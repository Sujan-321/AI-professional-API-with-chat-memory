# ğŸš€ RAG Backend with FastAPI, Qdrant, Redis & LLM

This project implements a **productionâ€‘grade Retrievalâ€‘Augmented Generation (RAG) backend** using FastAPI. It was developed as a hiring task and fulfills all requirements, including document ingestion, vector storage, multiâ€‘turn chat, and interview booking.

---

# ğŸ“Œ **Features**

## âœ… 1. Document Ingestion API

* Upload **PDF** or **TXT** files
* Extract text (PDF/TXT supported)
* Two chunking strategies:

  * **fixed** (wordâ€‘based)
  * **paragraph** (newline/paragraph based)
* Generate embeddings using **Sentence Transformers (allâ€‘MiniLMâ€‘L6â€‘v2)**
* Store vectors in **Qdrant** with metadata
* Store document info in **SQLite**

## âœ… 2. Conversational RAG API

* Custom RAG (**no RetrievalQAChain**)
* Multiâ€‘turn conversation memory stored in **Redis**
* Retrieves top chunks from Qdrant
* Builds custom prompt with context
* Uses LLM (Groq/OpenAI/etc.) for final answer
* Stores conversation history in Redis
* Fully stateless backend except for DB + Redis

## âœ… 3. Interview Booking

* User can provide name, email, date, time
* API saves booking in **SQLite (Booking model)**
* Returns confirmation message via LLM or direct logic

## ğŸ”’ Constraints (All Followed)

* âŒ No FAISS
* âŒ No Chroma
* âŒ No RetrievalQAChain
* âŒ No UI required
* âœ” Clean modular architecture
* âœ” Proper typing & structure

---

# ğŸ— **Project Architecture**

```
app/
 â”œâ”€â”€ api/
 â”‚    â”œâ”€â”€ document_ingestion.py
 â”‚    â””â”€â”€ conversate.py
 â”‚
 â”œâ”€â”€ services/
 â”‚    â”œâ”€â”€ chunking.py
 â”‚    â”œâ”€â”€ embedding.py
 â”‚    â”œâ”€â”€ rag_service.py
 â”‚    â”œâ”€â”€ text_extraction.py
 â”‚    â””â”€â”€ vector_db.py
 â”‚
 â”œâ”€â”€ db/
 â”‚    â”œâ”€â”€ database.py
 â”‚    â””â”€â”€ models.py
 â”‚
 â””â”€â”€ utils/
      â”œâ”€â”€ config.py
      â””â”€â”€ redis_client.py

main.py
requirements.txt
```

---

# ğŸ“¥ **Document Ingestion API**

`POST /api/upload`

### **Formâ€‘data fields:**

* `file`: PDF or TXT file
* `chunk_strategy`: `fixed` or `paragraph`

### **Response example:**

```json
{
  "message": "Document uploaded and processed successfully.",
  "document_id": 4,
  "filename": "universe.txt",
  "filetype": "txt",
  "chunk_strategy": "fixed",
  "total_chunks": 1,
  "vector_ids": ["ac905921-07ce-4a79-b806-94cc2c9b73ef"]
}
```

---

# ğŸ’¬ **Conversational RAG API**

`POST /api/conversate`

### **Request Body:**

```json
{
  "session_id": "123",
  "query": "How was the universe formed?",
  "top_k": 4
}
```

### **Features during conversation**

* Retrieves relevant document chunks
* Includes previous conversation context (Redis)
* Builds custom prompt
* Sends to LLM
* Saves back conversation

### **Response Example:**

```json
{
  "answer": "The universe formed after...",
  "sources": [ { "chunk": "...", "filename": "universe.txt" } ]
}
```

---

# ğŸ“† **Interview Booking Support**

### **Request Body:**

```json
{
  "session_id": "12345",
  "booking": {
    "name": "Sujan",
    "email": "sujan@mail.com",
    "date": "2025-01-10",
    "time": "14:00"
  }
}
```

### **Response:**

```json
{
  "answer": "Booking confirmed for Sujan on 2025-01-10 at 14:00 âœ…",
  "sources": []
}
```

---

# ğŸ§© Chunking Strategies

### **1. Fixed Chunking**

* 200â€“300 word slices
* Best for dense text

### **2. Paragraph Chunking**

* Split by blank lines
* Best for knowledge documents

Both methods are selectable via API.

---

# ğŸ§  Embeddings

Uses:

```
sentence-transformers/all-MiniLM-L6-v2 (384-dim)
```

Stored in Qdrant with metadata:

```
filename
chunk text
chunk_id
document_id
```

---

# ğŸ—„ Qdrant Vector Store

Collection name: `documents`

Vector schema:

* size = 384
* distance = COSINE

Upsert uses `PointStruct` with UUID-based IDs.

---

# ğŸ§± Redis (Conversation Memory)

Memory is stored per session:

```
session:<session_id>:history
```

Used for multi-turn chat.

---

# ğŸ›  Installation

### 1ï¸âƒ£ Clone repo

```
git clone <repo-url>
cd project-folder
```

### 2ï¸âƒ£ Create virtual environment

```
python -m venv venv
source venv/bin/activate     # Linux/macOS
venv\Scripts\activate       # Windows
```

### 3ï¸âƒ£ Install dependencies

```
pip install -r requirements.txt
```

### 4ï¸âƒ£ Start Qdrant

```
docker run -d --name qdrant -p 6333:6333 qdrant/qdrant
```

### 5ï¸âƒ£ Start Redis

```
docker run -d -p 6379:6379 redis
```

### 6ï¸âƒ£ Run FastAPI

```
uvicorn main:app --reload
```

---

# ğŸ§ª Testing

Visit Swagger:

```
http://localhost:8000/docs
```

Try:

* `/api/upload`
* `/api/conversate`

---

# ğŸ Troubleshooting

### **Error: limit argument in Qdrant search**

Solution implemented:

* Automatic fallback for older Qdrant client

### **Document returns 0 chunks**

* Ensure file has readable text
* Check paragraph mode (requires blank lines)

---

# ğŸ¯ Conclusion

This backend fully implements a modern RAG system with modular design, rich features, and production-ready architecture suitable for real business applications.

---

# ğŸ‘¨â€ğŸ’» Author

**Sujan Ghimire** â€” AI/ML Developer

---

# â­ Want to Improve?

Feel free to extend:

* Streaming responses
* Auth layer
* Support for DOCX
* Hybrid search (BM25 + vectors)
